\setsection{Vector Spaces}
\label{sec:vectorspace}

We have seen from the previous chapter that
matrices provide a way to express systems of linear equations.
In the process, we considered tuples
--- or \emph{vectors} ---
of unknowns, arguably in either \(\Rmath^n\) or \(\Cmath^n\).
Our goal is to give an appropriate abstraction of such spaces,
and analyze the key properties of them.

\subsection[Definition and Examples]%
    {Definition and Examples of Vector Spaces}
\label{sbc:vspdef}

Recall that our first usage of matrices
were to represent systems of linear equations.
There, numbers with a single index were written as vectors,
whereas numbers with double indices were written in a matrix.
Our first objective here
is to give an appropriate abstraction of such numbers.

\begin{example}
    \label{exm:vspintro}
    Note that we in fact have very limited examples of vectors:
    \(\Rmath^n\) and \(\Cmath^n\).
    We have established
    --- or \emph{intuitively} know ---
    the fact that \(\Rmath\) is a complete ordered field.
    It is also well-known that
    \(\Cmath\) is not an ordered field,
    but every polynomial with complex coefficients
    has a root in \(\Cmath\).

    Which properties should we focus on?
    Experience with \(\Cmath\) or \(\Rmath^n\) suggests that
    the order relation of \(\Rmath\) is barely relevant;
    we never really cared about ``positive'' entries of tuples.
    Similarly, we never used completeness in the sense that
    no Cauchy sequences were involved in our computations.
    Thus, one can deduce that
    the key part of matrix algebra is in the operations itself.
    Addition and multiplication
    --- with their companions subtraction and division ---
    of numbers is all we need to consider.

    However, it is not entirely clear whether
    we need all the four operations.
    For example, how can we multiply two \(n\)-tuples
    to obtain another \(n\)-tuple?
    On the other hand,
    multiplying a number and an \(n\)-tuple seems relatively easy.
\end{example}

\begin{example}[Physical Units]
    \label{exm:vspintro2}
    One of the most basic foundation of physics is the concept of units.
    We want to focus on the fact that
    units abide to the usual laws of arithmetic.
    For example, multiplying mass [kg] with velocity [m/s]
    we obtain momentum [kg m/s].

    Now consider masses \(m_1\) and \(m_2\) of two objects.
    We can add up the two and obtain an object with mass \(m_1+m_2\).
    Also, we can scale up the first object with resulting mass \(cm_1\).
    However, what do we obtain by multiplying \(m_1\) and \(m_2\)?
    We might obtain something,
    but we can no longer call it a mass of some object.
\end{example}

The two examples show that
to reach our current objective,
it is best to consider addition and scalar multiplication.
For simplicity, we want to impose some extra properties of operations.
This results in the following definition.

\begin{definition}[Vector Spaces]
    \label{def:vectorspace}
    Let \(F\) be a field.%
    \footnote{We have not yet defined what a field is;
    here, we consider fields as sets which behave like the real numbers
    under addition and multiplication.
    The most prominent examples are
    \(\Qmath\), \(\Rmath\) and \(\Cmath\).}
    Consider a set \(V\) and two operations
    \(+:V\times V\to V\), \(\cdot:F\times V\to V\)
    whose images are denoted by \(v+w\) and \(av\)
    instead of \(+(v,w)\) and \(\cdot(a,v)\).
    Suppose that all the following holds:
    \begin{axioms}[Vsp]
        \item For all \(u,v,w\) in \(V\), \((u+v)+w=u+(v+w)\);
        \hfill\textsf{(Associativity)}

        \item For all \(v,w\) in \(V\), \(v+w=w+v\);
        \hfill\textsf{(Commutativity)}

        \item There exists an element \(v_0\) in \(V\) such that
        \(v+v_0=v\) for all \(v\) in \(V\);%
        \footnote{
            Such an element is denoted by \(0\)
            instead of \(v_0\).}

        \hfill\textsf{(Additive identity)}

        \item For any \(v\) in \(V\),
        there exists an element \(\tilde v\) in \(V\) such that
        \(v+\tilde v=0\);%
        \footnote{
            Such an element is denoted by \(-v\)
            instead of \(\tilde v\).}

        \hfill\textsf{(Additive inverse)}

        \item For any \(v,w\) in \(V\) and \(a\in F\),
        \(a(v+w)=av+aw\);
        \hfill\textsf{(Distributivity)}

        \item For any \(v\) in \(V\) and \(a,b\in F\),
        \((a+b)v=av+bv\);
        \hfill\textsf{(Distributivity)}

        \item For any \(v\) in \(V\) and \(a,b\in F\).
        \(a(bv)=(ab)v\);
        \hfill\textsf{(Compatibility)}

        \item For any \(v\in V\), \(1v=v\);
    \end{axioms}
    then \(V\) is called
    a \define[space!vector space]{vector space over \(F\)}
    or an \textdef{\(F\)-vector space},
    and the elements of \(V\) are called
    \define[vector]{vectors}.
\end{definition}

Having our definition of vector spaces,
it is important to check if
our motivating examples in \cref{exm:vspintro} fit in the above.

\begin{example}[Revisited: \(\Rmath^n\) and \(\Cmath^n\)]
    \label{exm:vsintroverif}
    Here we verify that
    \(\Rmath^n\) and \(\Cmath^n\) satisfy the given axioms above.
    The proof is not long,
    since all the above axioms hold for \(\Rmath\) and \(\Cmath\).
    For \(n>1\), we can repeat the same argument component-wise.
\end{example}

Since we are considering general fields \(F\)
instead of \(\Rmath\) and \(\Cmath\),
we can define a vector space structure on the product \(F^n\).

\begin{definition}[Tuples as Vectors]
    \label{def:Fn}
    The set \(F^n=\set{a=(a^1,\dots,a^n)}{a^i\in F}\)
    of \(n\)-tuples of elements of \(F\)
    forms a vector space under component-wise operations;
    that is,
    \begin{align*}
        (a^1,\dots,a^n)+(b^1,\dots,b^n)
        &=(a^1+b^1,\dots,a^n+b^n) \\
        c(a^1,\dots,a^n)
        &=(ca^1,\dots,ca^n).
        \qquad\ (c\in F)
    \end{align*}
\end{definition}

Of course, there are more examples of vector spaces.
Most of the following examples consist of function spaces,
since those were the most required examples throughout history.

\begin{example}[Polynomials]
    \label{exm:vsppoly}
    Recall that polynomials of \(t\) over a field \(F\)
    are formal symbols of the form \(a_0+a_1t+\cdots+a_nt^n\).
    Denote the set of all such polynomials by \(\denote{\poly{F}}\).
    Then \(\poly{F}\) is a vector space over the field \(F\)
    under the usual operations.

    Moreover, let \(\denote{\finitepoly{n}{F}}\)
    denote the set of all polynomials of \(t\) over \(F\)
    of degree no greater than \(n\).
    Then \(\finitepoly{n}{F}\) also forms a vector space over \(F\).
\end{example}

Note that
\(\finitepoly{n}{F}\) can be manipulated just as \(\Rmath^{n+1}\)
if we consider polynomials \(a_0+a_1t+\cdots+a_nt^n\)
as \((n+1)\)-tuples \((a_0,\dots,a_n)\).
Viewing them as sequences \(\zseq{a_i}{i}\)
which terminate at most \(i=n\),
we can further extend this idea
and consider polynomials in \(\poly{F}\)
as finitely terminating sequences on \(F\).

\begin{example}[Function Spaces]
    \label{exm:vspfunc}
    Given nonempty sets \(X\) and \(Y\),
    we denote the set of all functions from \(X\) to \(Y\) by
    \(\denote{\func{X,Y}}\).
    If \(X\) and \(Y\) are mere sets,
    then there is little to say in the perspective of linear algebra.

    Things get more interesting if
    we let \(Y\) to be a vector space \(V\) over \(F\).
    Then, \(\func{X,V}\) is a vector space under point-wise operations:
    \[
        (f+g)(x)=f(x)+g(x),
        \qquad
        (cf)(x)=cf(x),
        \qquad
        (c\in F, x\in X)
    \]
    Of course, spaces of ``\(C^k\)-functions'' in analysis
    --- although we have not defined them yet ---
    are also vector spaces under point-wise operations.
\end{example}

Just a quick note;
the codomain \(Y\) in the notation \(\func{X,Y}\) can be omitted
if it is clear from the context.
Other several remarks can be made about \cref{exm:vspfunc}.

\begin{remark}[The Structure of the Codomain]
    \label{rmk:funcsp1}
    Consider a function \(f:X\to Y\).
    Originally, \(f\) is merely an assignment from \(X\) to \(Y\),
    and there is not much we can assert about \(f\).
    It is the fact that the codomain \(V\) is a vector space
    which makes \(\func{X,V}\) a vector space in the usual sense.
    This is a common strategy in studying functions:
    endowing the structure of the codomain to function spaces.
\end{remark}
\begin{remark}[Functions as Vectors]
    \label{rmk:funcsp2}
    Consider the \(n\)-tuple \(a=(2,4,\dots,2n)\) in \(\Rmath^n\).
    Then it is both natural to
    refer to the \(i\)-th component \(a^i\) of \(a\),
    and to see \(a\) as a function \(a:\{1,\dots,n\}\to\Rmath\)
    defined by \(i\mapsto a^i=2i\).
    Thus, we can call \(a(i)\) the \(i\)-th component of \(a\).

    Similarly, consider the sequence \(a:\Nmath\to\Rmath\)
    defined by \(i\mapsto 2i\).
    Extending the idea given above,
    it makes sense to call \(a(i)\) the \(i\)-th component of \(a\).
    Indeed, if we were to use infinite tuples,
    then the sequence \(a\) would be represented as \((2,4,6,\dots)\).

    The leap of faith is that
    given any function \(f:X\to Y\),
    we can call the image \(f(x)\) of \(x\in X\)
    the ``\(x\)-th component'' of \(f\).
    In this point of view,
    the point-wise operations in \(\func{X,V}\) makes sense;
    it is exactly what we have done in \(F^n\).
\end{remark}

\begin{example}[Matrices as Vectors]
    \label{exm:vspmat}
    In the previous chapter,
    matrices played the role of transforming vectors.
    However, it is also possible to view matrices themselves as vectors.

    Denote the set of all \mats{m}{n} with components in \(F\)
    by \(\denote{\Msp_{m,n}(F)}\).
    Then it is clearly a vector space under the usual operations.
    Note that we do not consider multiplications of matrices here;
    it is not in the definition of vector spaces.
\end{example}

\subsection[Properties]{Properties of Vector Spaces}
\label{sbc:vspprop}

So far, we have obtained several models of vector spaces,
most of them which are just variations of \(\func{X,V}\).
These are objects that have rather concrete definitions.
However, there is absolutely no information
about the abstract vector space \(V\) itself;
it is just a set with two operations.
What properties can we deduce about vector spaces?

In order to answer this question,
we must use the only information we know about \(V\);
addition and scalar multiplication.

\begin{definition*}[Linear Combinations]
    \label{def:lincombi}
    Let \(V\) be an \(F\)-vector space,
    and \(S=\{v_1,\dots,v_n\}\) be a finite subset of \(V\).
    A \define[linear combination]{linear combination of \(S\)}
    is a vector of the form
    \[
        a^1v_1+\cdots+a^nv_n
    \]
    where \(a^1,\dots,a^n\) are elements of \(F\).
\end{definition*}
